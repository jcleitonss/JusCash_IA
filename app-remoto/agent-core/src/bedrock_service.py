"""
Servi√ßo de integra√ß√£o com Amazon Bedrock (invoke_model direto)
"""
import boto3
import json
import os
from typing import Dict, Any

AWS_REGION = os.getenv('AWS_REGION', 'us-east-1')
MODEL_ID = "anthropic.claude-3-5-sonnet-20240620-v1:0"

# Cliente Bedrock Runtime (n√£o Agent)
bedrock_runtime = boto3.client(
    'bedrock-runtime',
    region_name=AWS_REGION
)

SYSTEM_PROMPT = """Voc√™ √© um analista jur√≠dico especializado em an√°lise de processos para compra de cr√©ditos.

Avalie o processo conforme as POL√çTICAS DE NEG√ìCIO:

**Regra-base (elegibilidade)**
POL-1: S√≥ compramos cr√©dito de processos transitados em julgado e em fase de execu√ß√£o (OBRIGAT√ìRIO)
POL-2: Exigir valor de condena√ß√£o informado (OBRIGAT√ìRIO)

**Quando N√ÉO compramos o cr√©dito**
POL-3: Valor de condena√ß√£o < R$ 1.000,00 ‚Üí REJEITAR
POL-4: Condena√ß√µes na esfera trabalhista ‚Üí REJEITAR
POL-5: √ìbito do autor sem habilita√ß√£o no invent√°rio ‚Üí REJEITAR
POL-6: Substabelecimento sem reserva de poderes ‚Üí REJEITAR

**Honor√°rios**
POL-7: Informar honor√°rios contratuais, periciais e sucumbenciais quando existirem (OBRIGAT√ìRIO)

**Qualidade**
POL-8: Se faltar documento essencial ‚Üí INCOMPLETO

Retorne APENAS um objeto JSON v√°lido, sem texto adicional:
{
  "decision": "approved|rejected|incomplete",
  "rationale": "Justificativa clara citando as pol√≠ticas",
  "citacoes": ["POL-X", "POL-Y"]
}"""


def invoke_bedrock_model(processo_dict: Dict[str, Any]) -> Dict[str, Any]:
    """
    Invoca Bedrock Model diretamente (sem Agent)
    
    Args:
        processo_dict: Dados do processo judicial
        
    Returns:
        Resposta com decision, rationale e citacoes
    """
    print("ü§ñ Invocando Bedrock Model:", MODEL_ID)
    
    # Monta prompt
    user_message = f"Analise o processo abaixo:\n\n{json.dumps(processo_dict, ensure_ascii=False, indent=2)}"
    
    # Payload para Claude
    payload = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 2000,
        "temperature": 0.1,
        "system": SYSTEM_PROMPT,
        "messages": [
            {
                "role": "user",
                "content": user_message
            }
        ]
    }
    
    # Invoca modelo
    response = bedrock_runtime.invoke_model(
        modelId=MODEL_ID,
        body=json.dumps(payload)
    )
    
    # Parse resposta
    response_body = json.loads(response['body'].read())
    
    # Extrai texto da resposta
    content = response_body.get('content', [])
    if content and len(content) > 0:
        result_text = content[0].get('text', '')
    else:
        result_text = ""
    
    print("‚úÖ Resposta recebida:", len(result_text), "caracteres")
    
    # Parse JSON
    try:
        # Remove markdown se houver
        if result_text.startswith('```json'):
            result_text = result_text.split('```json')[1].split('```')[0].strip()
        elif result_text.startswith('```'):
            result_text = result_text.split('```')[1].split('```')[0].strip()
        
        result_json = json.loads(result_text)
        return result_json
    except json.JSONDecodeError as e:
        print("‚ùå Erro ao parsear JSON:", str(e))
        print("Texto recebido:", result_text[:500])
        return {
            "decision": "incomplete",
            "rationale": f"Erro ao processar resposta do modelo: {str(e)}",
            "citacoes": ["POL-8"]
        }


def get_model_info() -> Dict[str, Any]:
    """
    Retorna informa√ß√µes sobre o modelo configurado
    """
    return {
        "model_id": MODEL_ID,
        "region": AWS_REGION,
        "configured": True
    }
